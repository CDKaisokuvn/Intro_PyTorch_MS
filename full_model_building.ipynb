{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "from torchvision import datasets\r\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\r\n",
    "import matplotlib.pyplot as plt \r\n",
    "from torch import nn "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "training_data = datasets.FashionMNIST(\r\n",
    "    root='data',\r\n",
    "    train=True,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")\r\n",
    "\r\n",
    "test_data = datasets.FashionMNIST(\r\n",
    "    root='data',\r\n",
    "    train=False,\r\n",
    "    download=True,\r\n",
    "    transform=ToTensor()\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\duong\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "training_dataloader = DataLoader(training_data, batch_size=64)\r\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create class of Model\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class NeuralNetwork(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super(NeuralNetwork, self).__init__()\r\n",
    "        self.flatten = nn.Flatten()\r\n",
    "        self.linear_relu_stack = nn.Sequential(\r\n",
    "            nn.Linear(28*28, 512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512,512),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Linear(512,10),\r\n",
    "            nn.ReLU()\r\n",
    "        )\r\n",
    "    def forward(self,x):\r\n",
    "        x = self.flatten(x)\r\n",
    "        logits = self.linear_relu_stack(x)\r\n",
    "        return logits\r\n",
    "\r\n",
    "model = NeuralNetwork().to(device)\r\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizing the Model Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\r\n",
    "learning_rate = 1e-2\r\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def training_loop(dataloader, model, loss_fn, optimizer):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    #batch is indice of (X,y)\r\n",
    "    for batch, (X,y) in enumerate(dataloader):\r\n",
    "        X,y = X.to(device), y.to(device)\r\n",
    "\r\n",
    "        # Compute prediction error\r\n",
    "\r\n",
    "        pred = model(X)\r\n",
    "        loss = loss_fn(pred,y)\r\n",
    "\r\n",
    "        # Back propagation\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "\r\n",
    "        if batch % 100 == 0:\r\n",
    "            loss, current = loss.item(), batch * len(X)\r\n",
    "            print(f'loss: {loss:>7f} [{current:>5d}/{size:>5d}]')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def test_loop(dataloader, model):\r\n",
    "    size = len(dataloader.dataset)\r\n",
    "    model.eval()\r\n",
    "    test_loss, correct = 0,0\r\n",
    "    with torch.no_grad():\r\n",
    "        for X,y in dataloader:\r\n",
    "            X,y = X.to(device), y.to(device)\r\n",
    "            pred = model(X)\r\n",
    "            test_loss += loss_fn(pred, y).item()\r\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\r\n",
    "    test_loss /=size\r\n",
    "    correct /= size\r\n",
    "\r\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "epochs = 15\r\n",
    "\r\n",
    "for t in range(epochs):\r\n",
    "    print(f\"Epoch {t+1}\\n -----------------------------------\")\r\n",
    "    training_loop(training_dataloader, model, loss_fn, optimizer)\r\n",
    "    test_loop(test_dataloader, model)\r\n",
    "print(\"\\n Done!!!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      " -----------------------------------\n",
      "loss: 2.301492 [    0/60000]\n",
      "loss: 2.240894 [ 6400/60000]\n",
      "loss: 2.090830 [12800/60000]\n",
      "loss: 1.914142 [19200/60000]\n",
      "loss: 2.024375 [25600/60000]\n",
      "loss: 1.878367 [32000/60000]\n",
      "loss: 1.610099 [38400/60000]\n",
      "loss: 1.615495 [44800/60000]\n",
      "loss: 1.647838 [51200/60000]\n",
      "loss: 1.530398 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 0.024452 \n",
      "\n",
      "Epoch 2\n",
      " -----------------------------------\n",
      "loss: 1.605896 [    0/60000]\n",
      "loss: 1.675449 [ 6400/60000]\n",
      "loss: 1.419198 [12800/60000]\n",
      "loss: 1.364275 [19200/60000]\n",
      "loss: 1.740939 [25600/60000]\n",
      "loss: 1.597412 [32000/60000]\n",
      "loss: 1.386551 [38400/60000]\n",
      "loss: 1.480685 [44800/60000]\n",
      "loss: 1.529063 [51200/60000]\n",
      "loss: 1.407685 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.5%, Avg loss: 0.022907 \n",
      "\n",
      "Epoch 3\n",
      " -----------------------------------\n",
      "loss: 1.477490 [    0/60000]\n",
      "loss: 1.590840 [ 6400/60000]\n",
      "loss: 1.340189 [12800/60000]\n",
      "loss: 1.304198 [19200/60000]\n",
      "loss: 1.691328 [25600/60000]\n",
      "loss: 1.535967 [32000/60000]\n",
      "loss: 1.334927 [38400/60000]\n",
      "loss: 1.450820 [44800/60000]\n",
      "loss: 1.486828 [51200/60000]\n",
      "loss: 1.335792 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 0.022071 \n",
      "\n",
      "Epoch 4\n",
      " -----------------------------------\n",
      "loss: 1.403770 [    0/60000]\n",
      "loss: 1.533723 [ 6400/60000]\n",
      "loss: 1.290307 [12800/60000]\n",
      "loss: 1.270190 [19200/60000]\n",
      "loss: 1.657246 [25600/60000]\n",
      "loss: 1.496206 [32000/60000]\n",
      "loss: 1.297702 [38400/60000]\n",
      "loss: 1.434864 [44800/60000]\n",
      "loss: 1.468331 [51200/60000]\n",
      "loss: 1.273077 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.9%, Avg loss: 0.021572 \n",
      "\n",
      "Epoch 5\n",
      " -----------------------------------\n",
      "loss: 1.356116 [    0/60000]\n",
      "loss: 1.502615 [ 6400/60000]\n",
      "loss: 1.262366 [12800/60000]\n",
      "loss: 1.244349 [19200/60000]\n",
      "loss: 1.635053 [25600/60000]\n",
      "loss: 1.472335 [32000/60000]\n",
      "loss: 1.268885 [38400/60000]\n",
      "loss: 1.428581 [44800/60000]\n",
      "loss: 1.451140 [51200/60000]\n",
      "loss: 1.226274 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.6%, Avg loss: 0.021281 \n",
      "\n",
      "Epoch 6\n",
      " -----------------------------------\n",
      "loss: 1.329014 [    0/60000]\n",
      "loss: 1.482326 [ 6400/60000]\n",
      "loss: 1.247417 [12800/60000]\n",
      "loss: 1.221167 [19200/60000]\n",
      "loss: 1.625687 [25600/60000]\n",
      "loss: 1.452482 [32000/60000]\n",
      "loss: 1.239623 [38400/60000]\n",
      "loss: 1.414524 [44800/60000]\n",
      "loss: 1.436632 [51200/60000]\n",
      "loss: 1.200044 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.1%, Avg loss: 0.021103 \n",
      "\n",
      "Epoch 7\n",
      " -----------------------------------\n",
      "loss: 1.312624 [    0/60000]\n",
      "loss: 1.468299 [ 6400/60000]\n",
      "loss: 1.235629 [12800/60000]\n",
      "loss: 1.203794 [19200/60000]\n",
      "loss: 1.617791 [25600/60000]\n",
      "loss: 1.439092 [32000/60000]\n",
      "loss: 1.218261 [38400/60000]\n",
      "loss: 1.403315 [44800/60000]\n",
      "loss: 1.423801 [51200/60000]\n",
      "loss: 1.182917 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Avg loss: 0.020991 \n",
      "\n",
      "Epoch 8\n",
      " -----------------------------------\n",
      "loss: 1.301018 [    0/60000]\n",
      "loss: 1.456892 [ 6400/60000]\n",
      "loss: 1.226043 [12800/60000]\n",
      "loss: 1.190207 [19200/60000]\n",
      "loss: 1.610280 [25600/60000]\n",
      "loss: 1.430433 [32000/60000]\n",
      "loss: 1.200147 [38400/60000]\n",
      "loss: 1.393842 [44800/60000]\n",
      "loss: 1.411602 [51200/60000]\n",
      "loss: 1.171340 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.020888 \n",
      "\n",
      "Epoch 9\n",
      " -----------------------------------\n",
      "loss: 1.292636 [    0/60000]\n",
      "loss: 1.449502 [ 6400/60000]\n",
      "loss: 1.216576 [12800/60000]\n",
      "loss: 1.180414 [19200/60000]\n",
      "loss: 1.599778 [25600/60000]\n",
      "loss: 1.420313 [32000/60000]\n",
      "loss: 1.186046 [38400/60000]\n",
      "loss: 1.383596 [44800/60000]\n",
      "loss: 1.399322 [51200/60000]\n",
      "loss: 1.165670 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.8%, Avg loss: 0.020748 \n",
      "\n",
      "Epoch 10\n",
      " -----------------------------------\n",
      "loss: 1.284366 [    0/60000]\n",
      "loss: 1.438732 [ 6400/60000]\n",
      "loss: 1.207993 [12800/60000]\n",
      "loss: 1.173120 [19200/60000]\n",
      "loss: 1.590715 [25600/60000]\n",
      "loss: 1.413053 [32000/60000]\n",
      "loss: 1.174524 [38400/60000]\n",
      "loss: 1.371200 [44800/60000]\n",
      "loss: 1.388387 [51200/60000]\n",
      "loss: 1.161050 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Avg loss: 0.020630 \n",
      "\n",
      "Epoch 11\n",
      " -----------------------------------\n",
      "loss: 1.279050 [    0/60000]\n",
      "loss: 1.429592 [ 6400/60000]\n",
      "loss: 1.197328 [12800/60000]\n",
      "loss: 1.166277 [19200/60000]\n",
      "loss: 1.582832 [25600/60000]\n",
      "loss: 1.405528 [32000/60000]\n",
      "loss: 1.166380 [38400/60000]\n",
      "loss: 1.362853 [44800/60000]\n",
      "loss: 1.378862 [51200/60000]\n",
      "loss: 1.157406 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Avg loss: 0.020544 \n",
      "\n",
      "Epoch 12\n",
      " -----------------------------------\n",
      "loss: 1.275911 [    0/60000]\n",
      "loss: 1.422172 [ 6400/60000]\n",
      "loss: 1.187160 [12800/60000]\n",
      "loss: 1.159167 [19200/60000]\n",
      "loss: 1.574880 [25600/60000]\n",
      "loss: 1.396952 [32000/60000]\n",
      "loss: 1.157361 [38400/60000]\n",
      "loss: 1.355439 [44800/60000]\n",
      "loss: 1.370808 [51200/60000]\n",
      "loss: 1.152641 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Avg loss: 0.020465 \n",
      "\n",
      "Epoch 13\n",
      " -----------------------------------\n",
      "loss: 1.272703 [    0/60000]\n",
      "loss: 1.414017 [ 6400/60000]\n",
      "loss: 1.177708 [12800/60000]\n",
      "loss: 1.152906 [19200/60000]\n",
      "loss: 1.567228 [25600/60000]\n",
      "loss: 1.390718 [32000/60000]\n",
      "loss: 1.149637 [38400/60000]\n",
      "loss: 1.350125 [44800/60000]\n",
      "loss: 1.362873 [51200/60000]\n",
      "loss: 1.148621 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Avg loss: 0.020403 \n",
      "\n",
      "Epoch 14\n",
      " -----------------------------------\n",
      "loss: 1.270264 [    0/60000]\n",
      "loss: 1.407393 [ 6400/60000]\n",
      "loss: 1.169972 [12800/60000]\n",
      "loss: 1.146923 [19200/60000]\n",
      "loss: 1.558469 [25600/60000]\n",
      "loss: 1.380359 [32000/60000]\n",
      "loss: 1.142402 [38400/60000]\n",
      "loss: 1.346108 [44800/60000]\n",
      "loss: 1.355922 [51200/60000]\n",
      "loss: 1.146361 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Avg loss: 0.020352 \n",
      "\n",
      "Epoch 15\n",
      " -----------------------------------\n",
      "loss: 1.270142 [    0/60000]\n",
      "loss: 1.401620 [ 6400/60000]\n",
      "loss: 1.161671 [12800/60000]\n",
      "loss: 1.141594 [19200/60000]\n",
      "loss: 1.551286 [25600/60000]\n",
      "loss: 1.373994 [32000/60000]\n",
      "loss: 1.135708 [38400/60000]\n",
      "loss: 1.342342 [44800/60000]\n",
      "loss: 1.352710 [51200/60000]\n",
      "loss: 1.143546 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.020280 \n",
      "\n",
      "\n",
      " Done!!!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "torch.save(model.state_dict(), \"data/model.pth\")\r\n",
    "print(\"Saved model state to model.pth\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved model state to model.pth\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "## Loading models\r\n",
    "model = NeuralNetwork()\r\n",
    "model.load_state_dict(torch.load(\"data/model.pth\"))\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make a prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "classes = [\r\n",
    "    \"T-shirt/top\",\r\n",
    "    \"Trouser\",\r\n",
    "    \"Pullover\",\r\n",
    "    \"Dress\",\r\n",
    "    \"Coat\",\r\n",
    "    \"Sandal\",\r\n",
    "    \"Shirt\",\r\n",
    "    \"Sneaker\",\r\n",
    "    \"Bag\",\r\n",
    "    \"Ankle boot\",\r\n",
    "]\r\n",
    "\r\n",
    "model.eval()\r\n",
    "x, y = test_data[0][0], test_data[0][1]\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    pred = model(x)\r\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\r\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0118, 0.0039, 0.0000, 0.0000, 0.0275,\n",
      "          0.0000, 0.1451, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000,\n",
      "          0.1059, 0.3294, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4667, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
      "          0.3451, 0.5608, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n",
      "          0.3647, 0.4157, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.2078,\n",
      "          0.5059, 0.4706, 0.5765, 0.6863, 0.6157, 0.6510, 0.5294, 0.6039,\n",
      "          0.6588, 0.5490, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0431, 0.5373,\n",
      "          0.5098, 0.5020, 0.6275, 0.6902, 0.6235, 0.6549, 0.6980, 0.5843,\n",
      "          0.5922, 0.5647, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
      "          0.0078, 0.0039, 0.0000, 0.0118, 0.0000, 0.0000, 0.4510, 0.4471,\n",
      "          0.4157, 0.5373, 0.6588, 0.6000, 0.6118, 0.6471, 0.6549, 0.5608,\n",
      "          0.6157, 0.6196, 0.0431, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.3490, 0.5451, 0.3529,\n",
      "          0.3686, 0.6000, 0.5843, 0.5137, 0.5922, 0.6627, 0.6745, 0.5608,\n",
      "          0.6235, 0.6627, 0.1882, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0157,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000, 0.3843, 0.5333, 0.4314, 0.4275,\n",
      "          0.4314, 0.6353, 0.5294, 0.5647, 0.5843, 0.6235, 0.6549, 0.5647,\n",
      "          0.6196, 0.6627, 0.4667, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0078, 0.0078, 0.0039, 0.0078, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.1020, 0.4235, 0.4588, 0.3882, 0.4353, 0.4588,\n",
      "          0.5333, 0.6118, 0.5255, 0.6039, 0.6039, 0.6118, 0.6275, 0.5529,\n",
      "          0.5765, 0.6118, 0.6980, 0.0000],\n",
      "         [0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824,\n",
      "          0.2078, 0.3608, 0.4588, 0.4353, 0.4039, 0.4510, 0.5059, 0.5255,\n",
      "          0.5608, 0.6039, 0.6471, 0.6667, 0.6039, 0.5922, 0.6039, 0.5608,\n",
      "          0.5412, 0.5882, 0.6471, 0.1686],\n",
      "         [0.0000, 0.0000, 0.0902, 0.2118, 0.2549, 0.2980, 0.3333, 0.4627,\n",
      "          0.5020, 0.4824, 0.4353, 0.4431, 0.4627, 0.4980, 0.4902, 0.5451,\n",
      "          0.5216, 0.5333, 0.6275, 0.5490, 0.6078, 0.6314, 0.5647, 0.6078,\n",
      "          0.6745, 0.6314, 0.7412, 0.2431],\n",
      "         [0.0000, 0.2667, 0.3686, 0.3529, 0.4353, 0.4471, 0.4353, 0.4471,\n",
      "          0.4510, 0.4980, 0.5294, 0.5333, 0.5608, 0.4941, 0.4980, 0.5922,\n",
      "          0.6039, 0.5608, 0.5804, 0.4902, 0.6353, 0.6353, 0.5647, 0.5412,\n",
      "          0.6000, 0.6353, 0.7686, 0.2275],\n",
      "         [0.2745, 0.6627, 0.5059, 0.4078, 0.3843, 0.3922, 0.3686, 0.3804,\n",
      "          0.3843, 0.4000, 0.4235, 0.4157, 0.4667, 0.4706, 0.5059, 0.5843,\n",
      "          0.6118, 0.6549, 0.7451, 0.7451, 0.7686, 0.7765, 0.7765, 0.7333,\n",
      "          0.7725, 0.7412, 0.7216, 0.1412],\n",
      "         [0.0627, 0.4941, 0.6706, 0.7373, 0.7373, 0.7216, 0.6706, 0.6000,\n",
      "          0.5294, 0.4706, 0.4941, 0.4980, 0.5725, 0.7255, 0.7647, 0.8196,\n",
      "          0.8157, 1.0000, 0.8196, 0.6941, 0.9608, 0.9882, 0.9843, 0.9843,\n",
      "          0.9686, 0.8627, 0.8078, 0.1922],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0471, 0.2627, 0.4157, 0.6431, 0.7255,\n",
      "          0.7804, 0.8235, 0.8275, 0.8235, 0.8157, 0.7451, 0.5882, 0.3216,\n",
      "          0.0314, 0.0000, 0.0000, 0.0000, 0.6980, 0.8157, 0.7373, 0.6863,\n",
      "          0.6353, 0.6196, 0.5922, 0.0431],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]) 9\n",
      "Predicted: \"Sneaker\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "b8476f40adbb31d316263111a5645b453d35d3e96db58041b0655f021c3b1406"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}